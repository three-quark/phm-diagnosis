#!/usr/bin/env python
# coding: utf-8

# In[1]:


import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath('./introduction.ipynb'))))


# In[2]:


import numpy as np
from numpy.random import binomial, multivariate_normal, normal, uniform
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor
import matplotlib.pyplot as plt
import pandas as pd

from ylearn.estimator_model.meta_learner import TLearner, XLearner
from ylearn.utils import to_df
get_ipython().run_line_magic('matplotlib', 'inline')


# In[3]:


v = np.random.normal(size=(1000, 10))
y = np.hstack([v[:, [0]] < 0, v[:, [0]] > 0])

data = to_df(v=v)
covariate = data.columns


# In[4]:


y


# In[5]:


from ylearn.effect_interpreter.policy_interpreter import PolicyInterpreter
pit = PolicyInterpreter(max_depth=2)
pit.fit(data=data, est_model=None, covariate=covariate, effect_array=y.astype(float))

pit_result = pit.interpret()


# In[6]:


for i in range(57, 59):
    print(f'the policy for the sample {i}\n --------------\n' + pit_result[f'sample_{i}'] + '\n')


# In[7]:


pit.predict(data)[:3]


# In[8]:


pit.decide(data)[:3]


# In[9]:


pit.plot()
plt.show()


# In[10]:


from ylearn.estimator_model.double_ml import DoubleML


# In[11]:


X = np.random.normal(size=(1000, 10))
T = np.random.binomial(2, .5, size=(1000,))
y = (X[:, 0]) * (T==1) + (-X[:, 0]) * (T==2) 
dt = to_df(v=X, treatment=T, outcome=y)
covariate = dt.columns[:10]


# In[12]:


xl = XLearner(model=RandomForestRegressor())
xl.fit(data=dt, outcome='outcome', treatment='treatment', covariate=covariate)


# In[13]:


pit1 = PolicyInterpreter(max_depth=2)
pit1.fit(
    data=data,
    est_model=xl,
)


# In[14]:


pit1.plot()
plt.show()


# In[15]:


pit1_result = pit1.interpret()
for i in range(57, 60):
    print(f'the policy for the sample {i}\n --------------\n' + pit1_result[f'sample_{i}'] + '\n')


# In[16]:


dml = DoubleML(
    x_model=RandomForestClassifier(),
    y_model=RandomForestRegressor(),
    is_discrete_treatment=True,
)
dml.fit(data=dt, outcome='outcome', treatment='treatment', covariate=covariate)


# In[17]:


pit2 = PolicyInterpreter(max_depth=2,)
pit2.fit(
    data=dt,
    est_model=dml,
    covariate=covariate,
)
pit2_result = pit2.interpret()

for i in range(57, 60):
    print(f'the policy for the sample {i}\n --------------\n' + pit2_result[f'sample_{i}'] + '\n')


# In[18]:


pit2.plot()
plt.show()


# In[19]:


pit3 = PolicyInterpreter(max_depth=2,)
pit3.fit(
    data=dt,
    est_model=dml,
    covariate=covariate,
    treatment_names=['t0', 't1', 't2']
)
pit3.plot()
plt.show()


# In[20]:


pit3.decide(data)[:3]


# ## Single binary treatment.
# 
# ### The dataset is generated by the following process
# Below we use the einstein notation to alleviate the headaches of specifiying dimensions of tensors.
# \begin{align*}
#     x & \sim \text{Bernoulli}(f(w)), \quad f(w) = \sigma(w_i \beta^i + \eta), \quad \eta\sim \text{Uniform}(-1, 1) \\
#     y & = x \theta(v^i) + \gamma_j w^j + \epsilon, \quad \epsilon \sim \text{Uniform}(-1, 1)\\
#     w & \sim \text{Normal}(0, I_{n_w})\\
#     v & \sim \text{Uniform}(0, 1)^{n_v}
# \end{align*}

# In[21]:


# Define DGP
def generate_data(n, d, controls_outcome, treatment_effect, propensity):
    v = multivariate_normal(np.zeros(d), np.diag(np.ones(d)), n)
    X = np.apply_along_axis(lambda x: binomial(1, propensity(x), 1)[0], 1, v)
    Y0 = np.apply_along_axis(lambda x: controls_outcome(x), 1, v)
    treat_effect = np.apply_along_axis(lambda x: treatment_effect(x), 1, v)
    Y = Y0 + treat_effect * X
    return (Y, X, v)


# In[22]:


def generate_controls_outcome(d):
    beta = uniform(-3, 3, d)
    return lambda x: np.dot(x, beta) + normal(0, 1)
treatment_effect = lambda x: (1 if x[1] > 0.1 else -0.5)*8
propensity = lambda x: (0.8 if (x[2]>-0.5 and x[2]<0.5) else 0.2)
d = 5
n = 1000
n_test = 250
controls_outcome = generate_controls_outcome(d)
X_test = multivariate_normal(np.zeros(d), np.diag(np.ones(d)), n_test)
delta = 6/n_test
X_test[:, 1] = np.arange(-3, 3, delta)


# In[23]:


y, x, w = generate_data(n, d, controls_outcome, treatment_effect, propensity)
data_dict = {
    'outcome': y,
    'treatment': x,
}
test_dict = {}
adjustment = []
for i in range(w.shape[1]):
    data_dict[f'w_{i}'] = w[:, i].squeeze()
    test_dict[f'w_{i}'] = X_test[:, i].squeeze()
    adjustment.append(f'w_{i}')
outcome = 'outcome'
treatment = 'treatment'
data = pd.DataFrame(data_dict)
test_data = pd.DataFrame(test_dict)


# In[24]:


t = TLearner(
    model=GradientBoostingRegressor()
)
t.fit(
    data=data,
    outcome=outcome,
    treatment=treatment,
    covariate=adjustment,
)
t_pred = t.estimate(data=test_data, quantity=None)


# In[25]:


from ylearn.effect_interpreter.policy_interpreter import PolicyInterpreter


# In[26]:


p = PolicyInterpreter(max_depth=2)
p.fit(
    data=data,
    est_model=t,
)
policy = p.interpret(data=test_data)


# In[27]:


for i in range(60, 62):
    print(f'the policy for the sample {i}\n --------------\n' + policy[f'sample_{i}'] + '\n')


# In[28]:


p.plot()
plt.show()


# In[29]:


for i in range(55, 60):
    print(f'the policy for the sample {i}\n --------------\n' + policy[f'sample_{i}'] + '\n')


# In[ ]:




