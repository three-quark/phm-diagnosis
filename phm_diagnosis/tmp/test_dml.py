#!/usr/bin/env python
# coding: utf-8

# In[1]:


import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath('./introduction.ipynb'))))


# In[2]:


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from itertools import product
from sklearn.linear_model import LinearRegression, MultiTaskElasticNetCV
from sklearn.ensemble import RandomForestRegressor,RandomForestClassifier
from sklearn.preprocessing import PolynomialFeatures

from ylearn.exp_dataset.exp_data import single_continuous_treatment, single_binary_treatment, multi_continuous_treatment
from ylearn.estimator_model.double_ml import DoubleML
from ylearn.estimator_model.utils import nd_kron


# ## Single continuous treatment.
# 
# ### The dataset is generated by the following process [1]
# Below we use the einstein notation to alleviate the headaches of specifiying dimensions of tensors.
# \begin{align*}
#     x & = \beta_i w^i \\
#     y & = x \theta(v^i) + \gamma_j w^j + \epsilon \\
#     w & \sim \text{Normal}(0, I_{n_w})\\
#     v & \sim \text{Uniform}(0, 1)^{n_v}
# \end{align*}

# In[3]:


train, val, treatment_effect = single_continuous_treatment()


# In[4]:


train


# In[5]:


adjustment = train.columns[:-4]
covariate = 'c_0'
outcome = 'outcome'
treatment = 'treatment'


# In[6]:


dml = DoubleML(
    x_model=RandomForestRegressor(),
    y_model=RandomForestRegressor(),
    cf_fold=3,
)
dml.fit(
    train,
    outcome,
    treatment,
    adjustment,
    covariate, 
)


# In[7]:


dml_poly={}
for dg in [3,5]:
    dml_t = DoubleML(
        x_model=RandomForestRegressor(),
        y_model=RandomForestRegressor(),
        cf_fold=3,    
        covariate_transformer = PolynomialFeatures(degree=dg,include_bias=False)
    )
    dml_t.fit(
        train,
        outcome,
        treatment,
        adjustment,
        covariate, 
    )
    dml_poly[dg] = dml_t


# In[8]:


def exp_(x): return np.exp(2*x)
dat = np.array(list(product(np.arange(0, 1, 0.01), repeat=1))).ravel()
data_test = pd.DataFrame({'c_0': dat})
true_te = np.array([exp_(xi) for xi in data_test[covariate]])
ested_te = dml.estimate(data_test).ravel()


# In[9]:


plt.figure(figsize=(10, 6))
plt.plot(dat, ested_te, label='dml estimated')
for dg, est in dml_poly.items():
    ested_te_dg = est.estimate(data_test).ravel()
    plt.plot(dat, ested_te_dg, label=f'dml estimated with Polynomial(degree={dg})')
plt.plot(dat, true_te, 'b--', label='true')
plt.legend()
plt.show()


# ## Single binary treatment.
# 
# ### The dataset is generated by the following process
# Below we use the einstein notation to alleviate the headaches of specifiying dimensions of tensors.
# \begin{align*}
#     x & \sim \text{Bernoulli}(f(w)), \quad f(w) = \sigma(w_i \beta^i + \eta), \quad \eta\sim \text{Uniform}(-1, 1) \\
#     y & = x \theta(v^i) + \gamma_j w^j + \epsilon, \quad \epsilon \sim \text{Uniform}(-1, 1)\\
#     w & \sim \text{Normal}(0, I_{n_w})\\
#     v & \sim \text{Uniform}(0, 1)^{n_v}
# \end{align*}

# In[10]:


train1, val1, treatment_effect1 = single_binary_treatment()
def exp_(x): return np.exp(2*x[0])
n = 1000
n_x = 4
v_test1 = np.random.uniform(0, 1, size=(n, n_x))
v_test1[:, 0] = np.linspace(0, 1, n)
data_test_dict = {
    'c_0': v_test1[:, 0],
    'c_1': v_test1[:, 1],
    'c_2': v_test1[:, 2],
    'c_3': v_test1[:, 3],
}
data_test1 = pd.DataFrame(data_test_dict)
true_te = np.array([exp_(x_i) for x_i in v_test1])


# In[11]:


adjustment1 = train1.columns[:-7]
covariate1 = train1.columns[-7:-3]
# t_effect1 = train1['t_effect']
treatment = 'treatment'
outcome = 'outcome'


# In[12]:


dml1 = DoubleML(
    x_model=RandomForestClassifier(),
    y_model=RandomForestRegressor(),
    cf_fold=1,
    is_discrete_treatment=True,
)
dml1.fit(
    data=train1,
    outcome=outcome,
    treatment=treatment,
    adjustment=adjustment1,
    covariate=covariate1,
)
predicted = dml1.estimate(data_test1)


# In[13]:


data_test1


# In[14]:


plt.figure(figsize=(20,5))
plt.subplot(1, 3, 1)
plt.plot(v_test1[:, 0], predicted, label='dml')
# plt.fill_between(v_test[:, 0], lb, ub, alpha=.4)
plt.plot(v_test1[:, 0], true_te, 'b--', label='true effect')
plt.ylabel('Treatment Effect')
plt.xlabel('x')
plt.legend()
plt.show()


# ## Multiple continuous treatment.
# 
# ### The dataset is generated by the following process
# Below we use the einstein notation to alleviate the headaches of specifiying dimensions of tensors.
# \begin{align*}
#     x & = \beta_i w^i + \eta, \quad \eta\sim \text{Uniform}(-1, 1) \\
#     y & = x \theta(v) + (x^2) \phi^j(v) + \gamma_k w^k + \epsilon, \quad \epsilon \sim \text{Uniform}(-1, 1)\\
#     w & \sim \text{Normal}(0, I_{n_w})\\
#     v & \sim \text{Uniform}(0, 1)^{n_v}
# \end{align*}
# 
# We let 
# \begin{equation}
#     \theta(v) = e^{2v}, \quad \phi(v) = \ln (3v)
# \end{equation}

# In[15]:


train2, val2, test = multi_continuous_treatment()
data_test2 = test[0]
true_te1 = test[1]
true_te2 = test[2]


# In[16]:


adjustment2 = train2.columns[:-10]
covariate2 = train2.columns[-10:-5]
treatment2 = train2.columns[-5: -3]
outcome2 = 'outcome'


# In[17]:


from sklearn.ensemble import GradientBoostingRegressor
from sklearn.multioutput import MultiOutputRegressor

dml2 = DoubleML(
    x_model=MultiOutputRegressor(GradientBoostingRegressor()),
    y_model=GradientBoostingRegressor(),
    cf_fold=1,
    is_discrete_treatment=False,
)
dml2.fit(
    data=train2,
    outcome=outcome2,
    treatment=treatment2,
    adjustment=adjustment2,
    covariate=covariate2,
)


# In[18]:


effect2 = dml2.estimate(data=data_test2)


# In[19]:


v_test = np.random.uniform(0, 1, size=(100, 5))
v_test[:, 0] = np.linspace(0, 1, 100)

plt.figure(figsize=(13, 6))
plt.plot(v_test[:, 0], effect2[:, :, 0], label='dml for treatment 1')
plt.plot(v_test[:, 0], effect2[:, :, 1], label='dml for treatment 2')
plt.plot(v_test[:, 0], true_te1, '--', label='True effect1')
plt.plot(v_test[:, 0], true_te2, '--', label='True effect2')
plt.ylabel("Treatment Effect")
plt.xlabel("x")
plt.legend()
plt.show()


# ### A trivial test of discrete outcome

# In[49]:


outcome_binary = ((1 / (1 + np.exp(- train1['outcome'].values))) >= 0.5).astype(int)
train_binary_outcome = train1.copy()
train_binary_outcome.outcome = outcome_binary
train_binary_outcome.head()


# In[50]:


dml_binary = DoubleML(
    x_model=RandomForestClassifier(),
    y_model=RandomForestClassifier(),
    cf_fold=1,
    is_discrete_treatment=True,
    is_discrete_outcome=True,
    proba_output=True,
)
dml_binary.fit(
    data=train_binary_outcome,
    outcome=outcome,
    treatment=treatment,
    adjustment=adjustment1,
    covariate=covariate1,)


# In[51]:


dml_binary.estimate(data_test1, target_outcome=1)


# ## Single continuous treatment observational data
# 
# valid estimation models include
# - DeepIV
# - DoubleML
# - CausalTree
# 
# We perform our test on Dominickâ€™s dataset, a popular historical dataset of store-level orange juice prices and sales provided by University of Chicago Booth School of Business. \textbf{We will reveal that lower income consumers are more price-sensive.}[1]
# 
# [1] https://github.com/microsoft/EconML/blob/main/notebooks/Double%20Machine%20Learning%20Examples.ipynb

# In[23]:


import os
import urllib.request

from sklearn.preprocessing import StandardScaler


# Data preprocessing

# In[24]:


file_name = "oj_large.csv"

if not os.path.isfile(file_name):
    urllib.request.urlretrieve("https://msalicedatapublic.blob.core.windows.net/datasets/OrangeJuice/oj_large.csv", file_name)

oj_data = pd.read_csv(file_name)
oj_data['price'] = np.log(oj_data['price'])


# In[25]:


oj_data.head()


# In[26]:


oj_data.columns


# In[27]:


outcome3 = 'logmove'
treatment3 = 'price'
adjustment3 = [c for c in oj_data.columns if c not in ['price', 'logmove', 'brand', 'week', 'store','INCOME']]
covariate3 = 'INCOME'


# In[28]:


x_ = oj_data[treatment3].values
y_ = oj_data[outcome3].values
scaler = StandardScaler()
w_ = scaler.fit_transform(oj_data[adjustment3].values)
w_prime = pd.get_dummies(oj_data[['brand']]).values
w = np.concatenate((w_, w_prime), axis=1)
v = scaler.fit_transform(oj_data[[covariate3]].values).ravel()


# In[29]:


adjustment3.append('brand')
transfomred_data_dict = {treatment3: x_,
                         outcome3: y_,
                         covariate3: v,}
for i, wwww in enumerate(adjustment3):
    transfomred_data_dict[wwww] = w[:, i].ravel()


# In[30]:


transfomred_data = pd.DataFrame(transfomred_data_dict)


# In[31]:


transfomred_data.head()


# In[32]:


min_income = -1
max_income = 1
delta = (1 - (-1)) / 100
v_test = np.arange(min_income, max_income + delta - 0.001, delta).reshape(-1,1)


# In[33]:


dml3 = DoubleML(
    x_model=RandomForestRegressor(),
    y_model=RandomForestRegressor(),
    cf_fold=4,
)
dml3.fit(
    data=transfomred_data,
    outcome=outcome3,
    treatment=treatment3,
    adjustment=adjustment3,
    covariate=covariate3,
)


# In[34]:


mx = RandomForestRegressor()
my = RandomForestRegressor()
wv = np.concatenate((w, v.reshape(-1, 1)), axis=1)
mx.fit(wv, x_)
my.fit(wv, y_)
yprime = y_ - my.predict(wv).reshape(y_.shape)
xhat = x_ - mx.predict(wv).reshape(x_.shape)
xprime = (xhat * v).reshape(-1, 1)
l = LinearRegression()
l.fit(xprime, yprime.reshape(-1, 1))
(l.coef_ - dml3.yx_model.coef_) / dml3.yx_model.coef_


# In[35]:


test_data = transfomred_data.copy(deep=True)[:v_test.shape[0]]
test_data[covariate3] = v_test.ravel()
effect3 = dml3.estimate(data=test_data)


# In[36]:


test_data.head()


# In[37]:


plt.figure(figsize=(10,6))
plt.plot(v_test, effect3.squeeze(), label="OJ Elasticity")
plt.xlabel(r'Scale(Income)')
plt.ylabel('Orange Juice Elasticity')
plt.legend()
plt.title("Orange Juice Elasticity vs Income")
plt.show()


# ## Multiple continuous treatment, multiple outcome observational data.

# In[38]:


oj_data = pd.read_csv(file_name)
oj_data['price'] = np.log(oj_data["price"])
groupbylist = ["store", "week", "AGE60", "EDUC", "ETHNIC", "INCOME",
               "HHLARGE", "WORKWOM", "HVAL150",
               "SSTRDIST", "SSTRVOL", "CPDIST5", "CPWVOL5"]
oj_data1 = pd.pivot_table(oj_data,index=groupbylist,
                          columns=oj_data.groupby(groupbylist).cumcount(),
                          values=['logmove', 'price'],
                          aggfunc='sum').reset_index()
oj_data1.columns = oj_data1.columns.map('{0[0]}{0[1]}'.format) 
oj_data1 = oj_data1.rename(index=str,
                           columns={"logmove0": "logmove_T",
                                    "logmove1": "logmove_M",
                                    "logmove2":"logmove_D",
                                    "price0":"price_T",
                                    "price1":"price_M",
                                    "price2":"price_D"})



# In[39]:


oj_data1.head()


# In[40]:


outcome4 = ['logmove_T', 'logmove_M', 'logmove_D']
treatment4 = ['price_T', 'price_M', 'price_D']
adjustment4 = [c for c in groupbylist if c not in ['week', 'store', 'INCOME']]
covariate4 = ['INCOME']


# In[41]:


scaler = StandardScaler()
w4 = scaler.fit_transform(oj_data1[adjustment4].values)
x4 = scaler.fit_transform(oj_data1[covariate4].values)


# In[42]:


for i, name in enumerate(adjustment4):
    oj_data1[name] = w4[:, i]
oj_data1[covariate4] = x4
oj_data1.head()


# In[43]:


oj_data1.columns


# In[44]:


from sklearn import linear_model
dml4 = DoubleML(
    x_model=MultiTaskElasticNetCV(),
    y_model=MultiTaskElasticNetCV(),
    yx_model=None,
    cf_fold=6,
)
dml4.fit(
    data=oj_data1,
    outcome=outcome4,
    treatment=treatment4,
    adjustment=adjustment4,
    covariate=covariate4,
)


# In[45]:


min_income = -1
max_income = 1
delta = (1 - (-1)) / 100
v_test = np.arange(min_income, max_income + delta - 0.001, delta).reshape(-1, 1)
test_data4 = oj_data1.copy(deep=True)[:v_test.shape[0]]
test_data4[covariate4] = v_test

effect4 = dml4.estimate(test_data4)


# In[46]:


oj_data1.columns


# In[47]:


plt.figure(figsize=(18, 10))
dic={0:"Tropicana", 1:"Minute.maid", 2:"Dominicks"}
for i in range(3):
    for j in range(3):
        plt.subplot(3, 3, 3 * i + j + 1)
        plt.plot(v_test, effect4[:, i, j],
                 color="C{}".format(str(3 * i + j)),
                 label="OJ Elasticity {} to {}".format(dic[j], dic[i]))
        plt.xlabel(r'Scale(Income)')
        plt.ylabel('Orange Juice Elasticity')
        plt.legend()
plt.suptitle("Orange Juice Elasticity vs Income", fontsize=16)
plt.show()


# In[48]:


xmodel = MultiTaskElasticNetCV()
xmodel.fit(oj_data1[['INCOME'] + adjustment4].values, oj_data1[treatment4])
x_hat = xmodel.predict(oj_data1[['INCOME']+adjustment4].values)
ymodel = MultiTaskElasticNetCV()
ymodel.fit(oj_data1[['INCOME']+adjustment4].values, oj_data1[outcome4])
yhat = ymodel.predict(oj_data1[['INCOME']+adjustment4].values)
# x_hat = dml4.x_hat_dict['paras'][0]
# y_hat = dml4.y_hat_dict['paras'][0]
xprime = nd_kron(oj_data1[treatment4].values - x_hat, oj_data1[['INCOME']].values)
yprime = oj_data1[outcome4].values - yhat
lr = linear_model.LinearRegression()
lr.fit(xprime, yprime)
c = lr.coef_
c

